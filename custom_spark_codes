sudo docker compose down --volumes --remove-orphans
sudo docker rmi -f $(sudo docker images -q)
sudo docker volume rm $(sudo docker volume ls -q)
sudo docker network prune -f
sudo docker system prune -a --volumes -f
sudo docker compose build --no-cache
sudo docker compose up -d



sudo docker exec -it hue bash -c "echo 'import desktop.conf; print(desktop.conf.DATABASE.ENGINE.get())' | /usr/share/hue/build/env/bin/hue shell"


$HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager
- above inside the nodemanger pod



create external table if not exists breweries (
  NUM int,
  NAME char(100),
  CITY char(100),
  STATE char(100),
  ID int)
ROW format delimited
fields terminated by ','
stored as textfile
location 'hdfs://namenode:9000/data/openbeer/breweries';



docker exec -it datanode ssh -i /home/hadoop/.ssh/id_rsa -o StrictHostKeyChecking=no hadoop@172.18.0.3





df=spark.read.json("hdfs://namenode:9000/data/yelp/yelpdataset/data/yelp_academic_dataset_user.json")
df2=df.select('user_id','friends')
df2.write.csv("hdfs://namenode:9000/data/output")


create database if not exists openbeer;
use openbeer;

create external table if not exists breweries (
  NUM int,
  NAME char(100),
  CITY char(100),
  STATE char(100),
  ID int)
ROW format delimited
fields terminated by ','
stored as textfile
location '/data/openbeer/breweries';



hdfs dfs -chown sparkuser:supergroup /data


spark-submit --driver-cores 1 --executor-cores 2 --num-executors 3  --executor-memory 1500 main.py /home/satish/Downloads/yelp_dataset/yelp_academic_dataset_user.json "/media/satish/New Volume/docker_data_2/spark_delta_hive_metastore/python_scripts/output"



minikube addons enable metrics-server
minikube addons enable ingress

for jhub 
kubectl port-forward svc/proxy-public 8081:80 -n jhub
kubectl port-forward jupyter-borrow 4040:4040 -n default

from pyspark.sql import SparkSession


driver_host_ip = socket.gethostbyname(socket.gethostname())

spark = SparkSession.builder \
    .appName("JupyterHub on K8s") \
    .master("spark://spark-master.default.svc.cluster.local:7077") \
    .config("spark.executor.memory", "2g") \
    .config("spark.executor.memoryOverhead", "512m") \
    .config("spark.driver.memory", "2g") \
    .config("spark.driver.memoryOverhead", "512m") \
    .config("spark.executor.cores", "1") \
    .config("spark.driver.cores", "1") \
    .config("spark.driver.host", driver_host_ip) \
    .config("spark.driver.port", "4040") \
    .config("spark.blockManager.port", "7078") \
    .config("spark.driver.bindAddress", "0.0.0.0") \
    .config("spark.executor.extraJavaOptions", "-Dlog4j.debug -Dlog4j.configuration=file:/home/spark/conf/log4j.properties") \
    .getOrCreate()

spark.range(1000).show()