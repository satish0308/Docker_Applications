services:
  postgres:
    image: postgres:13
    container_name: hive-metastore-postgres
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: hiveuser
      POSTGRES_PASSWORD: hivepassword
    volumes:
      - ./postgres/init-hive-metastore.sql:/docker-entrypoint-initdb.d/init-hive-metastore.sql
      - ./postgres/data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - hadoop-network

  pgadmin:
    image: dpage/pgadmin4
    container_name: pgadmin
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    volumes:
      - ./postgres/servers.json:/pgadmin4/servers.json
      - ./postgres/.pgpass:/var/lib/pgadmin/.pgpass  # âœ… Correct path
    ports:
      - "8081:80"
    depends_on:
      - postgres
    networks:
      - hadoop-network


  namenode:
    build:
      context: .
      dockerfile: hadoop.Dockerfile
    container_name: namenode
    hostname: namenode
    environment:
      - HDFS_NAMENODE_USER=hdfs
      - HDFS_DATANODE_USER=hdfs
      - HDFS_SECONDARYNAMENODE_USER=hdfs
      - CLUSTER_NAME=hadoop-cluster
      - HDFS_NAMENODE_USER=root
      - HADOOP_SSH_OPTS="-o StrictHostKeyChecking=no"
    volumes:
      - ./hadoop/namenode/data:/hadoop/dfs/name
    ports:
      - "9870:9870" # NameNode Web UI
      - "9000:9000" # NameNode RPC
    networks:
      - hadoop-network

  datanode:
    build:
      context: .
      dockerfile: hadoop.Dockerfile
    container_name: datanode
    hostname: datanode
    environment:
      - HDFS_NAMENODE_USER=hdfs
      - HDFS_DATANODE_USER=hdfs
      - HDFS_SECONDARYNAMENODE_USER=hdfs
    volumes:
      - ./hadoop/datanode/data:/hadoop/dfs/data
    ports:
      - "9864:9864" # DataNode Web UI
    networks:
      - hadoop-network

  hive:
    build:
      context: .
      dockerfile: hive.Dockerfile
    container_name: hive-server
    environment:
      - HDFS_NAMENODE_USER=hdfs
      - HDFS_DATANODE_USER=hdfs
      - HDFS_SECONDARYNAMENODE_USER=hdfs
      - POSTGRES_HOST=postgres
      - POSTGRES_DB=metastore
      - POSTGRES_USER=hiveuser
      - POSTGRES_PASSWORD=hivepassword
      - TMPDIR=/tmp/gunicorn
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "10000"]  # Check if HiveServer2 is running
      interval: 40s
      timeout: 50s
      retries: 5
    depends_on:
      - postgres
      - namenode
      - datanode
    ports:
      - "10000:10000" # HiveServer2
      - "10002:10002" # Hive Metastore Thrift
    networks:
      - hadoop-network

  hue:
    image: gethue/hue:latest
    container_name: hue
    user: "1001:1000"  # ðŸ”¹ Run as hue user instead of root
    environment:
      - TMPDIR=/tmp/gunicorn
      - HUE_CONF_DIR=/hue/desktop/conf
      - HUE_IGNORE_PASSWORD_SCRIPT_ERRORS=true
      - HUE_DATABASE=postgres
      - HUE_DATABASE_USER=hueuser
      - HUE_DATABASE_PASS=huepassword
      - HUE_DATABASE_DB=hue
      - HUE_SECRET_KEY=secret
    volumes:
      - ./hue:/hue/desktop/conf
      - ./hue/hue.ini:/usr/share/hue/desktop/conf/hue.ini
    ports:
      - "8888:8888"
    depends_on:
      hive:
        condition: service_healthy
    networks:
       - hadoop-network
  spark:
    build:
      context: .
      dockerfile: spark.Dockerfile
    container_name: spark
    depends_on:
      - postgres
      - namenode
      - datanode
    ports:
      - "4040:4040"
      - "4041:4041"
      - "18080:18080"
      - "5555:5555"
      - "8080:8080"
    networks:
      - hadoop-network



networks:
  hadoop-network:
    driver: bridge
